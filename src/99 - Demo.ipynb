{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5502c6-0163-442f-92dc-997dc6d44102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-17\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import pathlib\n",
    "import json\n",
    "from selenium.webdriver import Remote, ChromeOptions\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "import helpers\n",
    "from helpers import ai, scraping\n",
    "\n",
    "now = datetime.now(timezone.utc)\n",
    "today = now.strftime(\"%Y-%m-%d\")\n",
    "print(today)\n",
    "# today = \"2024-02-13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94730d-8554-4dbb-96d9-3deeb14ac797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e726f971-d685-4399-bfc1-aa6340ce10bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ChromeOptions()\n",
    "max_pages = 1\n",
    "today = \"2024-02-13\"\n",
    "\n",
    "# disable downloading images\n",
    "prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "now = datetime.now(timezone.utc)\n",
    "# today = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "url_pattern = \"https://news.ycombinator.com/front?day={day}&p={page}\"\n",
    "detail_pattern = \"https://news.ycombinator.com/item?id={item_id}\"\n",
    "sbr_connection = helpers.get_sbr_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "037c1aa9-c1dd-46d5-a9d2-b987890d4245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 https://news.ycombinator.com/front?day=2024-02-13&p=1\n"
     ]
    }
   ],
   "source": [
    "html_datas = []\n",
    "with Remote(sbr_connection, options=options) as driver:\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = url_pattern.format(day=today, page=page)\n",
    "        print(page, url)\n",
    "        driver.get(url) # HTTP GET\n",
    "        time.sleep(2)\n",
    "        html_source = driver.page_source \n",
    "        html_datas.append(html_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f14492-292a-4c18-b025-f7076377a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_and_keywords(content=\"\", client=None, raw=None):\n",
    "    system_prompt = \"\".join([\n",
    "        \"You are an expert web scraper and researcher.\",\n",
    "        \"When you get data, you perform expert-level summarization and keyword extraction.\",\n",
    "    ])\n",
    "    prompt_start = \"\".join([\n",
    "        \"Provide a concise summary of the contents of the following text with minimum of 3 paragraphs.\",\n",
    "        \"The summary should not include anything related to the discussion nature of the following text.\",\n",
    "        \"The summary should not include anything related to the conversation nature of the following text.\",\n",
    "        \"Also extract a 1-word subject of the following text as the top ranked keyword.\",\n",
    "        \"Extract and rank top keywords based on the subject matter of only of the following text.\",\n",
    "        \"Use the following text: \"\n",
    "    ])\n",
    "    prompt_end=\"Using format of \\\"{'summary': <generated-summary>, 'keywords': [{value: 'a', rank: 1}, {value: 'b', rank: 2}, {value: 'c', rank: 3}, {value: 'd', rank: 4}, {value: 'e', rank: 5}]}\\\" return a response with json\"\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"{prompt_start} {content} {prompt_end}\",\n",
    "        }\n",
    "    ]\n",
    "    return ai.perform_completion(messages, client=client, raw=raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71bb3071-1fc0-4342-b7cc-04641b1f2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_post_data(tr):\n",
    "    id = tr.attrs.get('id')\n",
    "    next_tr = tr.find_next('tr')\n",
    "    score_span = next_tr.find(\"span\", class_=\"score\")\n",
    "    score = None\n",
    "    if score_span:\n",
    "        score = \"\".join([x for x in score_span.get_text() if x.isdigit()])\n",
    "    title_element = tr.find(\"span\", class_=\"titleline\")\n",
    "    text = title_element.get_text()\n",
    "    target_links = [x.get('href') for x in tr.find_all('a') if x.get('href').startswith(\"http\")]\n",
    "    target_link = target_links[0] if len(target_links) >= 1 else None\n",
    "    detail_link = detail_pattern.format(item_id = id)\n",
    "    return  {\n",
    "            \"id\": id,\n",
    "            \"text\": text,\n",
    "            'target_link': target_link,\n",
    "            \"score\": score,\n",
    "            \"thread_link\": detail_link\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68c680-750f-4ac0-932d-6a5e4fcaa136",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for html_source in html_datas:\n",
    "    soup = BeautifulSoup(html_source, 'html.parser')\n",
    "    rows = soup.find_all('tr', class_=\"athing\")\n",
    "    with Remote(sbr_connection, options=options) as driver:\n",
    "        for tr in rows[:5]:\n",
    "            id = tr.attrs.get('id')\n",
    "            data = extract_post_data(tr)\n",
    "            thread_link = data.get('thread_link')\n",
    "            driver.get(thread_link)\n",
    "            thread_page_source = driver.page_source\n",
    "            soup = BeautifulSoup(thread_page_source, 'html.parser')\n",
    "            body = soup.find('body')\n",
    "            # use the parsed data\n",
    "            content = body.get_text()\n",
    "            content = content.replace('new | past | comments | ask | show | jobs | submit', '')\n",
    "            content = content.replace('login', '').replace('Hacker News', '')\n",
    "            content = content.replace('| hide | past | favorite |', '')\n",
    "            content = content.replace('| parent', '')\n",
    "            content = content.replace('| next [â€“] ', '')\n",
    "            content = content.strip()\n",
    "            content = content[:5000]\n",
    "            # using Ollama locally\n",
    "            pred_data, is_json = extract_summary_and_keywords(content=content)\n",
    "            dataset.append({\n",
    "                \"scraped\": data,\n",
    "                \"preds\": pred_data\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46d284-dd70-474d-bbe6-d0ef0aa73260",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d42ec-140b-42cc-bd1c-e168f0edd224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
